<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Kaushal Kumar Maurya</title>
    <meta name="author" content="Kaushal Kumar Maurya">
    <meta name="description" content="">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kaushal0494.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/Publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/Achievements/">Achievements</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/Miscellaneous/">Miscellaneous</a>
              </li>

              <!-- Resume -->
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/Kaushal_Resume.pdf" target="_blank">Resume</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Kaushal</span> Kumar Maurya
          </h1>
          <p class="desc">(he/him)| कौशल | கௌஷல் | スキル | కౌశల్ | Навык | ਹੁਨਰ |</p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/kaushal_pic.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="kaushal_pic.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%63%73%31%38%72%65%73%63%68%31%31%30%30%33@%69%69%74%68.%61%63.%69%6E" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=eMb2l_kAAAAJ&amp;hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/kaushal0494" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/kaushal-kumar-maurya-73016773" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/kaushalMaurya94" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            

              </div>

              <div class="contact-note">
                
              </div>
            </div>
          </div>

          <div class="clearfix">
            <p>I am a PhD candidate in the <a href="https://cse.iith.ac.in/" rel="external nofollow noopener" target="_blank">Department of Computer Science and Engineering</a> at <a href="https://www.iith.ac.in/" rel="external nofollow noopener" target="_blank">Indian Institute of Technology Hyderabad</a> and advised by Dr. <a href="https://www.iith.ac.in/~maunendra/" rel="external nofollow noopener" target="_blank">Maunendra Sankar Desarkar</a>. I am active collaborator at Microsoft India, working with <a href="https://sites.google.com/view/manishg/" rel="external nofollow noopener" target="_blank">Manish Gupta</a> and <a href="https://anoopkunchukuttan.gitlab.io/" rel="external nofollow noopener" target="_blank">Anoop Kunchukuttan</a>. I spent last three wonderful summers interning at <a href="https://www.microsoft.com/en-in/msidc/hyderabad-campus.aspx" rel="external nofollow noopener" target="_blank">Microsoft</a> (Translation and Auto-suggest teams) and <a href="https://resources.nvidia.com/en-us-gps-ai-capacity-building/nvaitc-research" rel="external nofollow noopener" target="_blank">Nvidia-AI</a>.</p>

<h3><a>Research Interest</a></h3>

<p>My research interest is applied Machine Learning for Natural Language Processing, with a particular focus on <em>Multilingual</em> Natural Language Processing. Simultaneously, I am also working towards improving knowledge augmentation and trustworthy generation with large language models.</p>

<p>As a multilingual myself, I perceive a pressing need for the development of cross-lingual/multilingual models that can facilitate a range of end-user applications in low-resource languages. In this regard, my research trajectory is primarily geared towards the development of novel models that can enable technology for low-resource languages that has limited or no data. To achieve this overarching objective, I am actively engaged in enhancing multilingual transfer learning models for language generation with limited supervision. My approach to addressing these issues is largely rooted in the linguistic perspective. In particular, my Ph.D. research focuses on anchoring three narrative properties, namely (1) language structure, (2) context, and (3) transferability from/to diverse typological languages. Furthermore, as the use of large language models has increased in recent years, it is crucial to ensure that these models are deployed safely and ethically for the audience. I believe that the aforementioned endeavors constitute a pivotal stride in the direction of accomplishing my research objective, which is to democratize and ensure sefe deployment of NLP technologies.</p>

<!-- My research intreast is applied Machine Learning techniques for Natural Language Processing specifically Multilingual  natural language processing.  As a multilingual myself, I envision a need for a cross-lingual/multilingual model that enables many end-user applications in low-resource languages (LRLs). Towards this trajectory, I am particularly interested in devloping novel models to enable technologies for low-resource languages with limited or no datset. With this overarching objective, I work towards improving multilingual transfer learning models for language generation with limited supervision. I have been mostly approaching issues from the linguistic side.  Specifically, my Ph.D. is focused on anchoring three narrative properties, which are (1) language structure, (2) context, and (3) transferability from/to diverse typological languages. I believe this is a step towards realizing my research mission to democrtize NLP technologies to diverse audiences. -->

<p>Prior to commencing my PhD studies, I served as a <em>Data Scientist</em> at <a href="https://ntwist.com/" rel="external nofollow noopener" target="_blank">NTWIST</a>, an AI-startup. Before this, I completed Master’s degree (M.Tech) in Artificial Intelligence from the <a href="https://www.uohyd.ac.in/" rel="external nofollow noopener" target="_blank">University of Hyderabad</a>  under the supervision of Prof. <a href="http://languagetechnologies.uohyd.ac.in/" rel="external nofollow noopener" target="_blank">K. Narayana Murthy</a>. I obtained Bachelor’s degree (B.Tech) in Computer Science and Engineering from Uttar Pradesh Technical University. During my PhD, I am humbled to recieve Suzuki foundation fellowship (two years consecutively) to visit <a href="https://www.shizuoka.ac.jp/" rel="external nofollow noopener" target="_blank">Shizuoka University, Japan</a>, for a short research stay.</p>

<p><!-- I worked in AI Startup i.e.,  as _data science analyst_. Prior to this, I completed my M.Tech in Artificial Intelligence from [University of Hyderabad](https://www.uohyd.ac.in/) under the guidance of Prof. [K. Narayana Murthy](http://languagetechnologies.uohyd.ac.in/) after completing my B.Tech from Uttar Pradesh Technical University. --></p>


          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">News</a></h2>          
          <div class="news">
            <div class="table-responsive" style="max-height: 10vw">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <th scope="row" style="width: 100px">May 2023</th>
                  <!-- <th scope="row" >May 2023</th> -->
                  <td>
                    Our paper “Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes” is accepted in the <strong>ECML-PKDD 2023 (Journal Track: DAMI)</strong>. his work is a collaborative effort with Microsoft India, supported by the <strong>Microsoft Partnership Grant</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 100px">May 2023</th>
                  <!-- <th scope="row" >May 2023</th> -->
                  <td>
                    Our paper “DIVHSK: Diverse Headline Generation using Self-Attention based Keyword Selection” is accepted in the <strong>Findings of ACL 2023</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 100px">Sep 2022</th>
                  <!-- <th scope="row" >Sep 2022</th> -->
                  <td>
                    Received a grant of Rs. 100k INR to attend conferences by IIT Hyderabad in <strong>Exceptional Research Scholar</strong> category.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 100px">May 2022</th>
                  <!-- <th scope="row" >May 2022</th> -->
                  <td>
                    Received <strong>Microsoft</strong> and <strong>ACL Travel Grants</strong> to attend ACL 2022 conference physically, Dublin Ireland.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" style="width: 100px">Mar 2022</th>
                  <!-- <th scope="row" >Mar 2022</th> -->
                  <td>
                    Our ZmBART and Meta-XNLG ACL papers are recognized as <strong>premier paper from India</strong> by <strong>IKDD-ACM India</strong>.
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <p> </p>
          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/trie_nlg.png"><abbr class="badge">Auto-Completion</abbr><abbr class="badge">Trie QAC</abbr><abbr class="badge">NLG Augmentation</abbr>
</div>

        <!-- Entry bib key -->
        <div id="maurya2023utilizing" class="col-sm-8">
        <!-- Title -->
        <div class="title">Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://kaushal0494.github.io/">Kaushal Kumar Maurya</a>, Maunendra Sankar Desarkar, Manish Gupta, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Puneet Agrawal' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In ECML-PKDD 2023 (Journal Track: DAMI)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/maurya2023_pkdd.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/kaushal0494/Trie-NLG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model by augmenting the prefix with rich context comprising of recent session queries and top trie completions. This simple modeling approach overcomes the limitations of trie-based and NLG-based approaches, and leads to state-of-the-art performance. We evaluate the Trie-NLG model using two large QAC datasets. On average, our model achieves huge 57% and 14% boost in MRR over the popular trie-based lookup and the strong BART-based baseline methods, respectively.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">maurya2023utilizing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar and Gupta, Manish and Agrawal, Puneet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECML-PKDD 2023 (Journal Track: DAMI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/divhsk.png"><abbr class="badge"><span>NLG</span></abbr><abbr class="badge"><span>Diverse Headlines</span></abbr><abbr class="badge"><span>Self-attention</span></abbr>
</div>

        <!-- Entry bib key -->
        <div id="maurya-desarkar-2023-divhsk" class="col-sm-8">
        <!-- Title -->
        <div class="title">DIVHSK: Diverse Headline Generation using Self-Attention based Keyword Selection</div>
        <!-- Author -->
        <div class="author">
        

        Venkatesh E, <a href="https://kaushal0494.github.io/">Kaushal Kumar Maurya</a>, Deepak Kumar, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Maunendra Sankar Desarkar' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Findings of the Association for Computational Linguistics: ACL 2023</em>, Jul 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://aclanthology.org/2023.findings-acl.118.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/kaushal0494/DivHSK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article, but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Towards this, we propose a novel model called DivHSK. It has two components: KeySelect for selecting the important keywords, and SeqGen, for finally generating the multiple diverse headlines. In KeySelect, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the most-attentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the SeqGen, a pre-trained encoder-decoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a state-of-the-art model. Additionally, We have also created a high-quality multi-reference headline dataset from news articles.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">maurya-desarkar-2023-divhsk</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DIVHSK: Diverse Headline Generation using Self-Attention based Keyword Selection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{E, Venkatesh and Maurya, Kaushal Kumar and Kumar, Deepak and Desarkar, Maunendra Sankar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: ACL 2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/metaxng.png"><abbr class="badge"><span>Cross-Lingual</span></abbr><abbr class="badge"><span>Meta-Learning</span></abbr><abbr class="badge"><span>Typology</span></abbr>
</div>

        <!-- Entry bib key -->
        <div id="maurya-desarkar-2022-meta" class="col-sm-8">
        <!-- Title -->
        <div class="title">Meta-X_NLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://kaushal0494.github.io/">Kaushal Kumar Maurya</a>, and Maunendra Desarkar</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Findings of the Association for Computational Linguistics: ACL 2022</em>, May 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://aclanthology.org/2022.findings-acl.24.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/kaushal0494/Meta_XNLG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          <a href="https://aclanthology.org/2022.findings-acl.24.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a>
            <a href="/assets/pdf/metaxng_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-doi="10.18653/v1/2022.findings-acl.24" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2022.findings-acl.24" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X_NLG) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">maurya-desarkar-2022-meta</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Maurya, Kaushal Kumar and Desarkar, Maunendra}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: ACL 2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Dublin, Ireland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-acl.24}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.findings-acl.24}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{269--284}</span><span class="p">,</span>
  <span class="na">talk</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-acl.24.mp4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/zmbart.png"><abbr class="badge"><span>Cross-Lingual</span></abbr><abbr class="badge"><span>Unsupervised</span></abbr><abbr class="badge"><span>Transfer-Learning</span></abbr>
</div>

        <!-- Entry bib key -->
        <div id="maurya-etal-2021-zmbart" class="col-sm-8">
        <!-- Title -->
        <div class="title">ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://kaushal0494.github.io/">Kaushal Kumar Maurya</a>, Maunendra Sankar Desarkar, Yoshinobu Kano, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Kumari Deepshikha' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, Aug 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://aclanthology.org/2021.findings-acl.248.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/kaushal0494/ZmBART" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          <a href="https://aclanthology.org/2021.findings-acl.248.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-doi="10.18653/v1/2021.findings-acl.248" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2021.findings-acl.248" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Despite the recent advancement in NLP research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from
high resource language (HRL) to multiple lowresource languages (LRLs) for natural language generation (NLG). We consider four
NLG tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., English, Hindi, and
Japanese. We propose an unsupervised crosslingual language generation framework (called
ZmBART) that does not use any parallel
or pseudo-parallel/back-translated data. In
this framework, we further pre-train mBART
sequence-to-sequence denoising auto-encoder
model with an auxiliary task using monolingual data of three languages. The objective
function of the auxiliary task is close to the
target tasks which enriches the multi-lingual
latent representation of mBART and provides
good initialization for target tasks. Then, this
model is fine-tuned with task-specific supervised English data and directly evaluated with
low-resource languages in the Zero-shot setting. To overcome catastrophic forgetting
and spurious correlation issues, we applied
freezing model component and data arguclosely related languages. We also show that the proposed character-span noise injection performs better than the unigram-character noise injection.mentation approaches respectively. This simple
modeling approach gave us promising results.
We experimented with few-shot training (with
1000 supervised data-points) which boosted
the model performance further. We performed
several ablations and cross-lingual transferability analysis to demonstrate the robustness of
ZmBART.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">maurya-etal-2021-zmbart</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Z}m{BART}: An Unsupervised Cross-lingual Transfer Framework for Language Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar and Kano, Yoshinobu and Deepshikha, Kumari}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.findings-acl.248}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.findings-acl.248}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2804--2818}</span><span class="p">,</span>
  <span class="na">talk</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.findings-acl.248.mp4}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/disgen.png"><abbr class="badge"><span>Q&amp;A, MCQ</span></abbr><abbr class="badge"><span>Multi-Decoder</span></abbr><abbr class="badge"><span>LSTM</span></abbr>
</div>

        <!-- Entry bib key -->
        <div id="10.1145/3340531.3411997" class="col-sm-8">
        <!-- Title -->
        <div class="title">Learning to Distract: A Hierarchical Multi-Decoder Network for Automated Generation of Long Distractors for Multiple-Choice Questions for Reading Comprehension</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://kaushal0494.github.io/">Kaushal Kumar Maurya</a>, and Maunendra Sankar Desarkar</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>, Aug 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/kaushal0494/HMD_Network" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-doi="10.1145/3340531.3411997" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-doi="10.1145/3340531.3411997" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The task of generating incorrect options for multiple-choice questions is termed as distractor generation problem. The task requires high cognitive skills and is extremely challenging to automate. Existing neural approaches for the task leverage encoder-decoder architecture to generate long distractors. However, in this process two critical points are ignored - firstly, many methods use Jaccard similarity over a pool of candidate distractors to sample the distractors. This often makes the generated distractors too obvious or not relevant to the question context. Secondly, some approaches did not consider the answer in the model, which caused the generated distractors to be either answer-revealing or semantically equivalent to the answer.In this paper, we propose a novel Hierarchical Multi-Decoder Network (HMD-Net) consisting of one encoder and three decoders, where each decoder generates a single distractor. To overcome the first problem mentioned above, we include multiple decoders with a dis-similarity loss in the loss function. To address the second problem, we exploit richer interaction between the article, question, and answer with a SoftSel operation and a Gated Mechanism. This enables the generation of distractors that are in context with questions but semantically not equivalent to the answers. The proposed model outperformed all the previous approaches significantly in both automatic and manual evaluations. In addition, we also consider linguistic features and BERT contextual embedding with our base model which further push the model performance.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3340531.3411997</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450368599}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3340531.3411997}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3340531.3411997}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 29th ACM International Conference on Information &amp;amp; Knowledge Management}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1115–1124}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{distractor generation, natural language generation, question-answering}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual Event, Ireland}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CIKM '20}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          <!-- Social -->
          <!--
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%63%73%31%38%72%65%73%63%68%31%31%30%30%33@%69%69%74%68.%61%63.%69%6E" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=eMb2l_kAAAAJ&hl=en" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/kaushal0494" title="GitHub"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/kaushal-kumar-maurya-73016773" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/kaushalMaurya94" title="Twitter"><i class="fab fa-twitter"></i></a>
            

              </div>

              <div class="contact-note">
                
              </div>
              
            </div> -->
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Kaushal Kumar Maurya. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll. </a> Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages. </a>Last updated: July 15, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
