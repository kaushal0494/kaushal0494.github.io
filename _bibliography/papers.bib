---
---

@inproceedings{maurya-desarkar-2022-meta,
    title = "Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation",
    author = "Maurya, Kaushal Kumar and
      Desarkar, Maunendra",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.24",
    doi = "10.18653/v1/2022.findings-acl.24",
    pdf="https://aclanthology.org/2022.findings-acl.24.pdf",
    pages = "269--284",
    selected={true},
    poster={metaxng_poster.pdf},
    %slides={slide.pdf},
    code={https://github.com/kaushal0494/Meta_XNLG},
    bibtex_show={true},
    talk={https://aclanthology.org/2022.findings-acl.24.mp4},
    abstract = "Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.",
}


@inproceedings{10.1145/3501247.3531579,
author = {Bagora, Aditi and Shrestha, Kamal and Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar},
title = {Hostility Detection in Online Hindi-English Code-Mixed Conversations},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3531579},
doi = {10.1145/3501247.3531579},
abstract = {With the rise in accessibility and popularity of various social media platforms, people have started expressing and communicating their ideas, opinions, and interests online. While these platforms are active sources of entertainment and idea-sharing, they also attract hostile and offensive content equally. Identification of hostile posts is an essential and challenging task. In particular, Hindi-English Code-Mixed online posts of conversational nature (which have a hierarchy of posts, comments, and replies) have escalated the challenges. There are two major challenges: (1) the complex structure of Code-Mixed text and (2) filtering the relevant previous context for a given utterance. To overcome these challenges, in this paper, we propose a novel hierarchical neural network architecture to identify hostile posts/comments/replies in online Hindi-English Code-Mixed conversations. We leverage large multilingual pre-trained (mLPT) models like mBERT, XLMR, and MuRIL. The mLPT models provide a rich representation of code-mix text and hierarchical modeling leads to a natural abstraction and selection of the relevant context. The propose model consistently outperformed all the baselines and emerged as a state-of-the-art performing model. We conducted multiple analyses and ablation studies to prove the robustness of the proposed model.},
booktitle = {14th ACM Web Science Conference 2022},
pages = {390â€“400},
numpages = {11},
keywords = {Neural networks, hostility detection, Code-Mixed data},
location = {Barcelona, Spain},
series = {WebSci '22},
talk={https://dl.acm.org/doi/10.1145/3501247.3531579},
pdf={https://dl.acm.org/doi/pdf/10.1145/3501247.3531579},
code={https://github.com/AditiBagora/Hasoc2021CodeMix},
bibtex_show={true},
slides={websci.pdf},
}

@inproceedings{maurya-etal-2021-zmbart,
    title = "{Z}m{BART}: An Unsupervised Cross-lingual Transfer Framework for Language Generation",
    author = "Maurya, Kaushal Kumar  and
      Desarkar, Maunendra Sankar  and
      Kano, Yoshinobu  and
      Deepshikha, Kumari",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.248",
    doi = "10.18653/v1/2021.findings-acl.248",
    pages = "2804--2818",
    pdf="https://aclanthology.org/2021.findings-acl.248.pdf",
    selected={true},
    %slides={slide.pdf},
    code={https://github.com/kaushal0494/ZmBART},
    bibtex_show={true},
    talk={https://aclanthology.org/2021.findings-acl.248.mp4},
    %dimensions={true},
    abstract ={Despite the recent advancement in NLP research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from
high resource language (HRL) to multiple lowresource languages (LRLs) for natural language generation (NLG). We consider four
NLG tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., English, Hindi, and
Japanese. We propose an unsupervised crosslingual language generation framework (called
ZmBART) that does not use any parallel
or pseudo-parallel/back-translated data. In
this framework, we further pre-train mBART
sequence-to-sequence denoising auto-encoder
model with an auxiliary task using monolingual data of three languages. The objective
function of the auxiliary task is close to the
target tasks which enriches the multi-lingual
latent representation of mBART and provides
good initialization for target tasks. Then, this
model is fine-tuned with task-specific supervised English data and directly evaluated with
low-resource languages in the Zero-shot setting. To overcome catastrophic forgetting
and spurious correlation issues, we applied
freezing model component and data argumentation approaches respectively. This simple
modeling approach gave us promising results.
We experimented with few-shot training (with
1000 supervised data-points) which boosted
the model performance further. We performed
several ablations and cross-lingual transferability analysis to demonstrate the robustness of
ZmBART.},
}

@article{DBLP:journals/corr/abs-2101-04998,
  author    = {Arkadipta De and
               Venkatesh Elangovan and
               Kaushal Kumar Maurya and
               Maunendra Sankar Desarkar},
  title     = {Coarse and Fine-Grained Hostility Detection in Hindi Posts using Fine
               Tuned Multilingual Embeddings},
  journal   = {CoRR},
  volume    = {abs/2101.04998},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.04998},
  eprinttype = {arXiv},
  eprint    = {2101.04998},
  timestamp = {Tue, 17 Aug 2021 16:23:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-04998.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract={Due to the wide adoption of social media platforms like Facebook, Twitter, etc., there is an emerging need of detecting online posts that can go against the community acceptance standards. The hostility detection task has been well explored for resource-rich languages like English, but is unexplored for resource-constrained languages like Hindi due to the unavailability of large suitable data. We view this hostility detection as a multi-label multi-class classification problem. We propose an effective neural network-based technique for hostility detection in Hindi posts. We leverage pre-trained multilingual Bidirectional Encoder Representations of Transformer (mBERT) to obtain the contextual representations of Hindi posts. We have performed extensive experiments including different pre-processing techniques, pre-trained models, neural architectures, hybrid strategies, etc. Our best performing neural classifier model includes One-vs-the-Rest approach where we obtained 92.60{\%}, 81.14{\%}, 69.59{\%}, 75.29{\%} and 73.01{\%} F1 scores for hostile, fake, hate, offensive, and defamation labels respectively. The proposed model (https://github.com/Arko98/Hostility-Detection-in-Hindi-Constraint-2021) outperformed the existing baseline models and emerged as the state-of-the-art model for detecting hostility in the Hindi posts.},
  pdf="https://www.springerprofessional.de/en/coarse-and-fine-grained-hostility-detection-in-hindi-posts-using/19047892",
  arxiv="https://arxiv.org/abs/2101.04998",
  code={https://github.com/Arko98/Hostility-Detection-in-Hindi-Constraint-2021},
  bibtex_show={true},
}

@inproceedings{10.1145/3340531.3411997,
author = {Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar},
title = {Learning to Distract: A Hierarchical Multi-Decoder Network for Automated Generation of Long Distractors for Multiple-Choice Questions for Reading Comprehension},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411997},
doi = {10.1145/3340531.3411997},
abstract = {The task of generating incorrect options for multiple-choice questions is termed as distractor generation problem. The task requires high cognitive skills and is extremely challenging to automate. Existing neural approaches for the task leverage encoder-decoder architecture to generate long distractors. However, in this process two critical points are ignored - firstly, many methods use Jaccard similarity over a pool of candidate distractors to sample the distractors. This often makes the generated distractors too obvious or not relevant to the question context. Secondly, some approaches did not consider the answer in the model, which caused the generated distractors to be either answer-revealing or semantically equivalent to the answer.In this paper, we propose a novel Hierarchical Multi-Decoder Network (HMD-Net) consisting of one encoder and three decoders, where each decoder generates a single distractor. To overcome the first problem mentioned above, we include multiple decoders with a dis-similarity loss in the loss function. To address the second problem, we exploit richer interaction between the article, question, and answer with a SoftSel operation and a Gated Mechanism. This enables the generation of distractors that are in context with questions but semantically not equivalent to the answers. The proposed model outperformed all the previous approaches significantly in both automatic and manual evaluations. In addition, we also consider linguistic features and BERT contextual embedding with our base model which further push the model performance.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1115â€“1124},
numpages = {10},
keywords = {distractor generation, natural language generation, question-answering},
location = {Virtual Event, Ireland},
series = {CIKM '20},
bibtex_show={true},
pdf="https://dl.acm.org/doi/pdf/10.1145/3340531.3411997",
selected={true},
%poster={metaxng_poster.pdf},
%slides={slide.pdf},
code={https://github.com/kaushal0494/HMD_Network},
bibtex_show={true},
%talk={https://aclanthology.org/2022.findings-acl.24.mp4},
}


@article{DBLP:journals/es/MadisettyMAD21,
  author    = {Sreekanth Madisetty and
               Kaushal Kumar Maurya and
               Akiko Aizawa and
               Maunendra Sankar Desarkar},
  title     = {A neural approach for detecting inline mathematical expressions from
               scientific documents},
  journal   = {Expert Syst. J. Knowl. Eng.},
  volume    = {38},
  number    = {4},
  year      = {2021},
  url       = {https://doi.org/10.1111/exsy.12576},
  doi       = {10.1111/exsy.12576},
  timestamp = {Tue, 13 Jul 2021 13:25:01 +0200},
  biburl    = {https://dblp.org/rec/journals/es/MadisettyMAD21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  bibtex_show={true},
  pdf={https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12576},
  abstract={Scientific documents generally contain multiple mathematical expressions in them. Detecting inline mathematical expressions are one of the most important and challenging tasks in scientific text mining. Recent works that detect inline mathematical expressions in scientific documents have looked at the problem from an image processing perspective. There is little work that has targeted the problem from NLP perspective. Towards this, we define a few features and applied Conditional Random Fields (CRF) to detect inline mathematical expressions in scientific documents. Apart from this feature based approach, we also propose a hybrid algorithm that combines Bidirectional Long Short Term Memory networks (Bi-LSTM) and feature-based approach for this task. Experimental results suggest that this proposed hybrid method outperforms several baselines in the literature and also individual methods in the hybrid approach.},
}

@inproceedings{maurya2020machine,
  title={Machine translation evaluation: Manual versus automaticâ€”a comparative study},
  author={Maurya, Kaushal Kumar and Ravindran, Renjith P and Anirudh, Ch Ram and Murthy, Kavi Narayana},
  booktitle={Data Engineering and Communication Technology: Proceedings of 3rd ICDECT-2K19},
  pages={541--553},
  year={2020},
  organization={Springer},
  bibtex_show={true},
  slides={mte.pdf},
  pdf={https://www.researchgate.net/profile/Kaushal-Maurya/publication/338467760_Machine_Translation_Evaluation_Manual_Versus_Automatic-A_Comparative_Study/links/5f4d1177458515a88b9a4ac8/Machine-Translation-Evaluation-Manual-Versus-Automatic-A-Comparative-Study.pdf},
  abstract={The quality of machine translation (MT) is best judged by humans well
versed in both source and target languages. However, automatic techniques are often
used as these are much faster, cheaper and language independent. The goal of this
paper is to check for correlation between manual and automatic evaluation, specifically in the context of Indian languages. To the extent automatic evaluation methods
correlate with the manual evaluations, we can get the best of both worlds. In this
paper, we perform a comparative study of automatic evaluation metricsâ€”BLEU,
NIST, METEOR, TER and WER, against the manual evaluation metric (adequacy),
for English-Hindi translation. We also attempt to estimate the manual evaluation
score of a given MT output from its automatic evaluation score. The data for the study
was sourced from the Workshop on Statistical Machine Translation WMT14.},

}


